{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning__artificial_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schota/shares/blob/master/Deep_Learning__artificial_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP6JLo1tGNBg",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UxeF0S8R4_a",
        "colab_type": "text"
      },
      "source": [
        "This one was for one of the keggle competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZyYmS_UE_L",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUT9Xi4_skO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaABAoE2szCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "040edbb4-e1a6-414d-fa1a-5692188a23eb"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0Q3aoKUCRX",
        "colab_type": "text"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKWAkFVGUU0Z",
        "colab_type": "text"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9qxoOcut4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('rawtrain.csv')\n",
        "X = dataset.iloc[:, 4:8].values\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlsHmDwpP7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset = pd.read_csv('rawtest.csv')\n",
        "X_test = testset.iloc[:, 3:7].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v9aJF6Lpj6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "3c840f0e-4614-440a-cbe7-e8d3dd738212"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['male', 22.0, 1, 0],\n",
              "       ['female', 38.0, 1, 0],\n",
              "       ['female', 26.0, 0, 0],\n",
              "       ...,\n",
              "       ['female', nan, 1, 2],\n",
              "       ['male', 26.0, 0, 0],\n",
              "       ['male', 32.0, 0, 0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suP_lo2wpn1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "819577c4-d84e-47a6-d0b6-039a91d4a757"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['male', 34.5, 0, 0],\n",
              "       ['female', 47.0, 1, 0],\n",
              "       ['male', 62.0, 0, 0],\n",
              "       ...,\n",
              "       ['male', 38.5, 0, 0],\n",
              "       ['male', nan, 0, 0],\n",
              "       ['male', nan, 1, 1]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPLyIUH5rGPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29bd6fd4-2ba8-4210-daa5-a313a2ac783a"
      },
      "source": [
        "X[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.0, 38.0, 26.0, 35.0, 35.0, nan, 54.0, 2.0, 27.0, 14.0, 4.0,\n",
              "       58.0, 20.0, 39.0, 14.0, 55.0, 2.0, nan, 31.0, nan, 35.0, 34.0,\n",
              "       15.0, 28.0, 8.0, 38.0, nan, 19.0, nan, nan, 40.0, nan, nan, 66.0,\n",
              "       28.0, 42.0, nan, 21.0, 18.0, 14.0, 40.0, 27.0, nan, 3.0, 19.0, nan,\n",
              "       nan, nan, nan, 18.0, 7.0, 21.0, 49.0, 29.0, 65.0, nan, 21.0, 28.5,\n",
              "       5.0, 11.0, 22.0, 38.0, 45.0, 4.0, nan, nan, 29.0, 19.0, 17.0, 26.0,\n",
              "       32.0, 16.0, 21.0, 26.0, 32.0, 25.0, nan, nan, 0.83, 30.0, 22.0,\n",
              "       29.0, nan, 28.0, 17.0, 33.0, 16.0, nan, 23.0, 24.0, 29.0, 20.0,\n",
              "       46.0, 26.0, 59.0, nan, 71.0, 23.0, 34.0, 34.0, 28.0, nan, 21.0,\n",
              "       33.0, 37.0, 28.0, 21.0, nan, 38.0, nan, 47.0, 14.5, 22.0, 20.0,\n",
              "       17.0, 21.0, 70.5, 29.0, 24.0, 2.0, 21.0, nan, 32.5, 32.5, 54.0,\n",
              "       12.0, nan, 24.0, nan, 45.0, 33.0, 20.0, 47.0, 29.0, 25.0, 23.0,\n",
              "       19.0, 37.0, 16.0, 24.0, nan, 22.0, 24.0, 19.0, 18.0, 19.0, 27.0,\n",
              "       9.0, 36.5, 42.0, 51.0, 22.0, 55.5, 40.5, nan, 51.0, 16.0, 30.0,\n",
              "       nan, nan, 44.0, 40.0, 26.0, 17.0, 1.0, 9.0, nan, 45.0, nan, 28.0,\n",
              "       61.0, 4.0, 1.0, 21.0, 56.0, 18.0, nan, 50.0, 30.0, 36.0, nan, nan,\n",
              "       9.0, 1.0, 4.0, nan, nan, 45.0, 40.0, 36.0, 32.0, 19.0, 19.0, 3.0,\n",
              "       44.0, 58.0, nan, 42.0, nan, 24.0, 28.0, nan, 34.0, 45.5, 18.0, 2.0,\n",
              "       32.0, 26.0, 16.0, 40.0, 24.0, 35.0, 22.0, 30.0, nan, 31.0, 27.0,\n",
              "       42.0, 32.0, 30.0, 16.0, 27.0, 51.0, nan, 38.0, 22.0, 19.0, 20.5,\n",
              "       18.0, nan, 35.0, 29.0, 59.0, 5.0, 24.0, nan, 44.0, 8.0, 19.0, 33.0,\n",
              "       nan, nan, 29.0, 22.0, 30.0, 44.0, 25.0, 24.0, 37.0, 54.0, nan,\n",
              "       29.0, 62.0, 30.0, 41.0, 29.0, nan, 30.0, 35.0, 50.0, nan, 3.0,\n",
              "       52.0, 40.0, nan, 36.0, 16.0, 25.0, 58.0, 35.0, nan, 25.0, 41.0,\n",
              "       37.0, nan, 63.0, 45.0, nan, 7.0, 35.0, 65.0, 28.0, 16.0, 19.0, nan,\n",
              "       33.0, 30.0, 22.0, 42.0, 22.0, 26.0, 19.0, 36.0, 24.0, 24.0, nan,\n",
              "       23.5, 2.0, nan, 50.0, nan, nan, 19.0, nan, nan, 0.92, nan, 17.0,\n",
              "       30.0, 30.0, 24.0, 18.0, 26.0, 28.0, 43.0, 26.0, 24.0, 54.0, 31.0,\n",
              "       40.0, 22.0, 27.0, 30.0, 22.0, nan, 36.0, 61.0, 36.0, 31.0, 16.0,\n",
              "       nan, 45.5, 38.0, 16.0, nan, nan, 29.0, 41.0, 45.0, 45.0, 2.0, 24.0,\n",
              "       28.0, 25.0, 36.0, 24.0, 40.0, nan, 3.0, 42.0, 23.0, nan, 15.0,\n",
              "       25.0, nan, 28.0, 22.0, 38.0, nan, nan, 40.0, 29.0, 45.0, 35.0, nan,\n",
              "       30.0, 60.0, nan, nan, 24.0, 25.0, 18.0, 19.0, 22.0, 3.0, nan, 22.0,\n",
              "       27.0, 20.0, 19.0, 42.0, 1.0, 32.0, 35.0, nan, 18.0, 1.0, 36.0, nan,\n",
              "       17.0, 36.0, 21.0, 28.0, 23.0, 24.0, 22.0, 31.0, 46.0, 23.0, 28.0,\n",
              "       39.0, 26.0, 21.0, 28.0, 20.0, 34.0, 51.0, 3.0, 21.0, nan, nan, nan,\n",
              "       33.0, nan, 44.0, nan, 34.0, 18.0, 30.0, 10.0, nan, 21.0, 29.0,\n",
              "       28.0, 18.0, nan, 28.0, 19.0, nan, 32.0, 28.0, nan, 42.0, 17.0,\n",
              "       50.0, 14.0, 21.0, 24.0, 64.0, 31.0, 45.0, 20.0, 25.0, 28.0, nan,\n",
              "       4.0, 13.0, 34.0, 5.0, 52.0, 36.0, nan, 30.0, 49.0, nan, 29.0, 65.0,\n",
              "       nan, 50.0, nan, 48.0, 34.0, 47.0, 48.0, nan, 38.0, nan, 56.0, nan,\n",
              "       0.75, nan, 38.0, 33.0, 23.0, 22.0, nan, 34.0, 29.0, 22.0, 2.0, 9.0,\n",
              "       nan, 50.0, 63.0, 25.0, nan, 35.0, 58.0, 30.0, 9.0, nan, 21.0, 55.0,\n",
              "       71.0, 21.0, nan, 54.0, nan, 25.0, 24.0, 17.0, 21.0, nan, 37.0,\n",
              "       16.0, 18.0, 33.0, nan, 28.0, 26.0, 29.0, nan, 36.0, 54.0, 24.0,\n",
              "       47.0, 34.0, nan, 36.0, 32.0, 30.0, 22.0, nan, 44.0, nan, 40.5,\n",
              "       50.0, nan, 39.0, 23.0, 2.0, nan, 17.0, nan, 30.0, 7.0, 45.0, 30.0,\n",
              "       nan, 22.0, 36.0, 9.0, 11.0, 32.0, 50.0, 64.0, 19.0, nan, 33.0, 8.0,\n",
              "       17.0, 27.0, nan, 22.0, 22.0, 62.0, 48.0, nan, 39.0, 36.0, nan,\n",
              "       40.0, 28.0, nan, nan, 24.0, 19.0, 29.0, nan, 32.0, 62.0, 53.0,\n",
              "       36.0, nan, 16.0, 19.0, 34.0, 39.0, nan, 32.0, 25.0, 39.0, 54.0,\n",
              "       36.0, nan, 18.0, 47.0, 60.0, 22.0, nan, 35.0, 52.0, 47.0, nan,\n",
              "       37.0, 36.0, nan, 49.0, nan, 49.0, 24.0, nan, nan, 44.0, 35.0, 36.0,\n",
              "       30.0, 27.0, 22.0, 40.0, 39.0, nan, nan, nan, 35.0, 24.0, 34.0,\n",
              "       26.0, 4.0, 26.0, 27.0, 42.0, 20.0, 21.0, 21.0, 61.0, 57.0, 21.0,\n",
              "       26.0, nan, 80.0, 51.0, 32.0, nan, 9.0, 28.0, 32.0, 31.0, 41.0, nan,\n",
              "       20.0, 24.0, 2.0, nan, 0.75, 48.0, 19.0, 56.0, nan, 23.0, nan, 18.0,\n",
              "       21.0, nan, 18.0, 24.0, nan, 32.0, 23.0, 58.0, 50.0, 40.0, 47.0,\n",
              "       36.0, 20.0, 32.0, 25.0, nan, 43.0, nan, 40.0, 31.0, 70.0, 31.0,\n",
              "       nan, 18.0, 24.5, 18.0, 43.0, 36.0, nan, 27.0, 20.0, 14.0, 60.0,\n",
              "       25.0, 14.0, 19.0, 18.0, 15.0, 31.0, 4.0, nan, 25.0, 60.0, 52.0,\n",
              "       44.0, nan, 49.0, 42.0, 18.0, 35.0, 18.0, 25.0, 26.0, 39.0, 45.0,\n",
              "       42.0, 22.0, nan, 24.0, nan, 48.0, 29.0, 52.0, 19.0, 38.0, 27.0,\n",
              "       nan, 33.0, 6.0, 17.0, 34.0, 50.0, 27.0, 20.0, 30.0, nan, 25.0,\n",
              "       25.0, 29.0, 11.0, nan, 23.0, 23.0, 28.5, 48.0, 35.0, nan, nan, nan,\n",
              "       36.0, 21.0, 24.0, 31.0, 70.0, 16.0, 30.0, 19.0, 31.0, 4.0, 6.0,\n",
              "       33.0, 23.0, 48.0, 0.67, 28.0, 18.0, 34.0, 33.0, nan, 41.0, 20.0,\n",
              "       36.0, 16.0, 51.0, nan, 30.5, nan, 32.0, 24.0, 48.0, 57.0, nan,\n",
              "       54.0, 18.0, nan, 5.0, nan, 43.0, 13.0, 17.0, 29.0, nan, 25.0, 25.0,\n",
              "       18.0, 8.0, 1.0, 46.0, nan, 16.0, nan, nan, 25.0, 39.0, 49.0, 31.0,\n",
              "       30.0, 30.0, 34.0, 31.0, 11.0, 0.42, 27.0, 31.0, 39.0, 18.0, 39.0,\n",
              "       33.0, 26.0, 39.0, 35.0, 6.0, 30.5, nan, 23.0, 31.0, 43.0, 10.0,\n",
              "       52.0, 27.0, 38.0, 27.0, 2.0, nan, nan, 1.0, nan, 62.0, 15.0, 0.83,\n",
              "       nan, 23.0, 18.0, 39.0, 21.0, nan, 32.0, nan, 20.0, 16.0, 30.0,\n",
              "       34.5, 17.0, 42.0, nan, 35.0, 28.0, nan, 4.0, 74.0, 9.0, 16.0, 44.0,\n",
              "       18.0, 45.0, 51.0, 24.0, nan, 41.0, 21.0, 48.0, nan, 24.0, 42.0,\n",
              "       27.0, 31.0, nan, 4.0, 26.0, 47.0, 33.0, 47.0, 28.0, 15.0, 20.0,\n",
              "       19.0, nan, 56.0, 25.0, 33.0, 22.0, 28.0, 25.0, 39.0, 27.0, 19.0,\n",
              "       nan, 26.0, 32.0], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6gkEeUvp8o8",
        "colab_type": "text"
      },
      "source": [
        "##Taking Care of Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzhbYyX1p_EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:4])\n",
        "X[:, 1:4] = imputer.transform(X[:, 1:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnmpS5FNqfmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef9f89b8-8203-4715-c619-eb6b9f635e15"
      },
      "source": [
        "X[:, 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.0, 38.0, 26.0, 35.0, 35.0, 29.69911764705882, 54.0, 2.0, 27.0,\n",
              "       14.0, 4.0, 58.0, 20.0, 39.0, 14.0, 55.0, 2.0, 29.69911764705882,\n",
              "       31.0, 29.69911764705882, 35.0, 34.0, 15.0, 28.0, 8.0, 38.0,\n",
              "       29.69911764705882, 19.0, 29.69911764705882, 29.69911764705882,\n",
              "       40.0, 29.69911764705882, 29.69911764705882, 66.0, 28.0, 42.0,\n",
              "       29.69911764705882, 21.0, 18.0, 14.0, 40.0, 27.0, 29.69911764705882,\n",
              "       3.0, 19.0, 29.69911764705882, 29.69911764705882, 29.69911764705882,\n",
              "       29.69911764705882, 18.0, 7.0, 21.0, 49.0, 29.0, 65.0,\n",
              "       29.69911764705882, 21.0, 28.5, 5.0, 11.0, 22.0, 38.0, 45.0, 4.0,\n",
              "       29.69911764705882, 29.69911764705882, 29.0, 19.0, 17.0, 26.0, 32.0,\n",
              "       16.0, 21.0, 26.0, 32.0, 25.0, 29.69911764705882, 29.69911764705882,\n",
              "       0.83, 30.0, 22.0, 29.0, 29.69911764705882, 28.0, 17.0, 33.0, 16.0,\n",
              "       29.69911764705882, 23.0, 24.0, 29.0, 20.0, 46.0, 26.0, 59.0,\n",
              "       29.69911764705882, 71.0, 23.0, 34.0, 34.0, 28.0, 29.69911764705882,\n",
              "       21.0, 33.0, 37.0, 28.0, 21.0, 29.69911764705882, 38.0,\n",
              "       29.69911764705882, 47.0, 14.5, 22.0, 20.0, 17.0, 21.0, 70.5, 29.0,\n",
              "       24.0, 2.0, 21.0, 29.69911764705882, 32.5, 32.5, 54.0, 12.0,\n",
              "       29.69911764705882, 24.0, 29.69911764705882, 45.0, 33.0, 20.0, 47.0,\n",
              "       29.0, 25.0, 23.0, 19.0, 37.0, 16.0, 24.0, 29.69911764705882, 22.0,\n",
              "       24.0, 19.0, 18.0, 19.0, 27.0, 9.0, 36.5, 42.0, 51.0, 22.0, 55.5,\n",
              "       40.5, 29.69911764705882, 51.0, 16.0, 30.0, 29.69911764705882,\n",
              "       29.69911764705882, 44.0, 40.0, 26.0, 17.0, 1.0, 9.0,\n",
              "       29.69911764705882, 45.0, 29.69911764705882, 28.0, 61.0, 4.0, 1.0,\n",
              "       21.0, 56.0, 18.0, 29.69911764705882, 50.0, 30.0, 36.0,\n",
              "       29.69911764705882, 29.69911764705882, 9.0, 1.0, 4.0,\n",
              "       29.69911764705882, 29.69911764705882, 45.0, 40.0, 36.0, 32.0, 19.0,\n",
              "       19.0, 3.0, 44.0, 58.0, 29.69911764705882, 42.0, 29.69911764705882,\n",
              "       24.0, 28.0, 29.69911764705882, 34.0, 45.5, 18.0, 2.0, 32.0, 26.0,\n",
              "       16.0, 40.0, 24.0, 35.0, 22.0, 30.0, 29.69911764705882, 31.0, 27.0,\n",
              "       42.0, 32.0, 30.0, 16.0, 27.0, 51.0, 29.69911764705882, 38.0, 22.0,\n",
              "       19.0, 20.5, 18.0, 29.69911764705882, 35.0, 29.0, 59.0, 5.0, 24.0,\n",
              "       29.69911764705882, 44.0, 8.0, 19.0, 33.0, 29.69911764705882,\n",
              "       29.69911764705882, 29.0, 22.0, 30.0, 44.0, 25.0, 24.0, 37.0, 54.0,\n",
              "       29.69911764705882, 29.0, 62.0, 30.0, 41.0, 29.0, 29.69911764705882,\n",
              "       30.0, 35.0, 50.0, 29.69911764705882, 3.0, 52.0, 40.0,\n",
              "       29.69911764705882, 36.0, 16.0, 25.0, 58.0, 35.0, 29.69911764705882,\n",
              "       25.0, 41.0, 37.0, 29.69911764705882, 63.0, 45.0, 29.69911764705882,\n",
              "       7.0, 35.0, 65.0, 28.0, 16.0, 19.0, 29.69911764705882, 33.0, 30.0,\n",
              "       22.0, 42.0, 22.0, 26.0, 19.0, 36.0, 24.0, 24.0, 29.69911764705882,\n",
              "       23.5, 2.0, 29.69911764705882, 50.0, 29.69911764705882,\n",
              "       29.69911764705882, 19.0, 29.69911764705882, 29.69911764705882,\n",
              "       0.92, 29.69911764705882, 17.0, 30.0, 30.0, 24.0, 18.0, 26.0, 28.0,\n",
              "       43.0, 26.0, 24.0, 54.0, 31.0, 40.0, 22.0, 27.0, 30.0, 22.0,\n",
              "       29.69911764705882, 36.0, 61.0, 36.0, 31.0, 16.0, 29.69911764705882,\n",
              "       45.5, 38.0, 16.0, 29.69911764705882, 29.69911764705882, 29.0, 41.0,\n",
              "       45.0, 45.0, 2.0, 24.0, 28.0, 25.0, 36.0, 24.0, 40.0,\n",
              "       29.69911764705882, 3.0, 42.0, 23.0, 29.69911764705882, 15.0, 25.0,\n",
              "       29.69911764705882, 28.0, 22.0, 38.0, 29.69911764705882,\n",
              "       29.69911764705882, 40.0, 29.0, 45.0, 35.0, 29.69911764705882, 30.0,\n",
              "       60.0, 29.69911764705882, 29.69911764705882, 24.0, 25.0, 18.0, 19.0,\n",
              "       22.0, 3.0, 29.69911764705882, 22.0, 27.0, 20.0, 19.0, 42.0, 1.0,\n",
              "       32.0, 35.0, 29.69911764705882, 18.0, 1.0, 36.0, 29.69911764705882,\n",
              "       17.0, 36.0, 21.0, 28.0, 23.0, 24.0, 22.0, 31.0, 46.0, 23.0, 28.0,\n",
              "       39.0, 26.0, 21.0, 28.0, 20.0, 34.0, 51.0, 3.0, 21.0,\n",
              "       29.69911764705882, 29.69911764705882, 29.69911764705882, 33.0,\n",
              "       29.69911764705882, 44.0, 29.69911764705882, 34.0, 18.0, 30.0, 10.0,\n",
              "       29.69911764705882, 21.0, 29.0, 28.0, 18.0, 29.69911764705882, 28.0,\n",
              "       19.0, 29.69911764705882, 32.0, 28.0, 29.69911764705882, 42.0, 17.0,\n",
              "       50.0, 14.0, 21.0, 24.0, 64.0, 31.0, 45.0, 20.0, 25.0, 28.0,\n",
              "       29.69911764705882, 4.0, 13.0, 34.0, 5.0, 52.0, 36.0,\n",
              "       29.69911764705882, 30.0, 49.0, 29.69911764705882, 29.0, 65.0,\n",
              "       29.69911764705882, 50.0, 29.69911764705882, 48.0, 34.0, 47.0, 48.0,\n",
              "       29.69911764705882, 38.0, 29.69911764705882, 56.0,\n",
              "       29.69911764705882, 0.75, 29.69911764705882, 38.0, 33.0, 23.0, 22.0,\n",
              "       29.69911764705882, 34.0, 29.0, 22.0, 2.0, 9.0, 29.69911764705882,\n",
              "       50.0, 63.0, 25.0, 29.69911764705882, 35.0, 58.0, 30.0, 9.0,\n",
              "       29.69911764705882, 21.0, 55.0, 71.0, 21.0, 29.69911764705882, 54.0,\n",
              "       29.69911764705882, 25.0, 24.0, 17.0, 21.0, 29.69911764705882, 37.0,\n",
              "       16.0, 18.0, 33.0, 29.69911764705882, 28.0, 26.0, 29.0,\n",
              "       29.69911764705882, 36.0, 54.0, 24.0, 47.0, 34.0, 29.69911764705882,\n",
              "       36.0, 32.0, 30.0, 22.0, 29.69911764705882, 44.0, 29.69911764705882,\n",
              "       40.5, 50.0, 29.69911764705882, 39.0, 23.0, 2.0, 29.69911764705882,\n",
              "       17.0, 29.69911764705882, 30.0, 7.0, 45.0, 30.0, 29.69911764705882,\n",
              "       22.0, 36.0, 9.0, 11.0, 32.0, 50.0, 64.0, 19.0, 29.69911764705882,\n",
              "       33.0, 8.0, 17.0, 27.0, 29.69911764705882, 22.0, 22.0, 62.0, 48.0,\n",
              "       29.69911764705882, 39.0, 36.0, 29.69911764705882, 40.0, 28.0,\n",
              "       29.69911764705882, 29.69911764705882, 24.0, 19.0, 29.0,\n",
              "       29.69911764705882, 32.0, 62.0, 53.0, 36.0, 29.69911764705882, 16.0,\n",
              "       19.0, 34.0, 39.0, 29.69911764705882, 32.0, 25.0, 39.0, 54.0, 36.0,\n",
              "       29.69911764705882, 18.0, 47.0, 60.0, 22.0, 29.69911764705882, 35.0,\n",
              "       52.0, 47.0, 29.69911764705882, 37.0, 36.0, 29.69911764705882, 49.0,\n",
              "       29.69911764705882, 49.0, 24.0, 29.69911764705882,\n",
              "       29.69911764705882, 44.0, 35.0, 36.0, 30.0, 27.0, 22.0, 40.0, 39.0,\n",
              "       29.69911764705882, 29.69911764705882, 29.69911764705882, 35.0,\n",
              "       24.0, 34.0, 26.0, 4.0, 26.0, 27.0, 42.0, 20.0, 21.0, 21.0, 61.0,\n",
              "       57.0, 21.0, 26.0, 29.69911764705882, 80.0, 51.0, 32.0,\n",
              "       29.69911764705882, 9.0, 28.0, 32.0, 31.0, 41.0, 29.69911764705882,\n",
              "       20.0, 24.0, 2.0, 29.69911764705882, 0.75, 48.0, 19.0, 56.0,\n",
              "       29.69911764705882, 23.0, 29.69911764705882, 18.0, 21.0,\n",
              "       29.69911764705882, 18.0, 24.0, 29.69911764705882, 32.0, 23.0, 58.0,\n",
              "       50.0, 40.0, 47.0, 36.0, 20.0, 32.0, 25.0, 29.69911764705882, 43.0,\n",
              "       29.69911764705882, 40.0, 31.0, 70.0, 31.0, 29.69911764705882, 18.0,\n",
              "       24.5, 18.0, 43.0, 36.0, 29.69911764705882, 27.0, 20.0, 14.0, 60.0,\n",
              "       25.0, 14.0, 19.0, 18.0, 15.0, 31.0, 4.0, 29.69911764705882, 25.0,\n",
              "       60.0, 52.0, 44.0, 29.69911764705882, 49.0, 42.0, 18.0, 35.0, 18.0,\n",
              "       25.0, 26.0, 39.0, 45.0, 42.0, 22.0, 29.69911764705882, 24.0,\n",
              "       29.69911764705882, 48.0, 29.0, 52.0, 19.0, 38.0, 27.0,\n",
              "       29.69911764705882, 33.0, 6.0, 17.0, 34.0, 50.0, 27.0, 20.0, 30.0,\n",
              "       29.69911764705882, 25.0, 25.0, 29.0, 11.0, 29.69911764705882, 23.0,\n",
              "       23.0, 28.5, 48.0, 35.0, 29.69911764705882, 29.69911764705882,\n",
              "       29.69911764705882, 36.0, 21.0, 24.0, 31.0, 70.0, 16.0, 30.0, 19.0,\n",
              "       31.0, 4.0, 6.0, 33.0, 23.0, 48.0, 0.67, 28.0, 18.0, 34.0, 33.0,\n",
              "       29.69911764705882, 41.0, 20.0, 36.0, 16.0, 51.0, 29.69911764705882,\n",
              "       30.5, 29.69911764705882, 32.0, 24.0, 48.0, 57.0, 29.69911764705882,\n",
              "       54.0, 18.0, 29.69911764705882, 5.0, 29.69911764705882, 43.0, 13.0,\n",
              "       17.0, 29.0, 29.69911764705882, 25.0, 25.0, 18.0, 8.0, 1.0, 46.0,\n",
              "       29.69911764705882, 16.0, 29.69911764705882, 29.69911764705882,\n",
              "       25.0, 39.0, 49.0, 31.0, 30.0, 30.0, 34.0, 31.0, 11.0, 0.42, 27.0,\n",
              "       31.0, 39.0, 18.0, 39.0, 33.0, 26.0, 39.0, 35.0, 6.0, 30.5,\n",
              "       29.69911764705882, 23.0, 31.0, 43.0, 10.0, 52.0, 27.0, 38.0, 27.0,\n",
              "       2.0, 29.69911764705882, 29.69911764705882, 1.0, 29.69911764705882,\n",
              "       62.0, 15.0, 0.83, 29.69911764705882, 23.0, 18.0, 39.0, 21.0,\n",
              "       29.69911764705882, 32.0, 29.69911764705882, 20.0, 16.0, 30.0, 34.5,\n",
              "       17.0, 42.0, 29.69911764705882, 35.0, 28.0, 29.69911764705882, 4.0,\n",
              "       74.0, 9.0, 16.0, 44.0, 18.0, 45.0, 51.0, 24.0, 29.69911764705882,\n",
              "       41.0, 21.0, 48.0, 29.69911764705882, 24.0, 42.0, 27.0, 31.0,\n",
              "       29.69911764705882, 4.0, 26.0, 47.0, 33.0, 47.0, 28.0, 15.0, 20.0,\n",
              "       19.0, 29.69911764705882, 56.0, 25.0, 33.0, 22.0, 28.0, 25.0, 39.0,\n",
              "       27.0, 19.0, 29.69911764705882, 26.0, 32.0], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCRfBtfLqUAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer_test = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer_test.fit(X_test[:, 1:4])\n",
        "X_test[:, 1:4] = imputer_test.transform(X_test[:, 1:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6bQ0UgSU-NJ",
        "colab_type": "text"
      },
      "source": [
        "### Encoding categorical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TMXiv--rV5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X[:, 0] = le.fit_transform(X[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPnUbHy-ritj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "b0a5a420-ee9b-49d2-af81-1e1fc381d144"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 22.0, 1.0, 0.0],\n",
              "       [0, 38.0, 1.0, 0.0],\n",
              "       [0, 26.0, 0.0, 0.0],\n",
              "       ...,\n",
              "       [0, 29.69911764705882, 1.0, 2.0],\n",
              "       [1, 26.0, 0.0, 0.0],\n",
              "       [1, 32.0, 0.0, 0.0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbOXXb0Vrk9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X_test[:, 0] = le.fit_transform(X_test[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHol938cW8zd",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPTLJFzewkVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE_FcHyfV3TQ",
        "colab_type": "text"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13F62awvwq-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reydbj_-sCpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "592d6fa5-a99a-49aa-924d-d48dc1571cc9"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.73769513,  0.36944878, -0.4745452 , -0.47367361],\n",
              "       [-1.35557354,  1.33137817,  0.43279337, -0.47367361],\n",
              "       [ 0.73769513,  2.48569343, -0.4745452 , -0.47367361],\n",
              "       ...,\n",
              "       [ 0.73769513,  0.67726619, -0.4745452 , -0.47367361],\n",
              "       [ 0.73769513,  0.04413122, -0.4745452 , -0.47367361],\n",
              "       [ 0.73769513,  0.04413122,  0.43279337,  0.76762988]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zfEzkRVXIwF",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - Building the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvdeScabXtlB",
        "colab_type": "text"
      },
      "source": [
        "### Initializing the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1o_snEtx5Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP6urV6SX7kS",
        "colab_type": "text"
      },
      "source": [
        "### Adding the input layer and the first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNc2KQyRyfhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BELWAc_8YJze",
        "colab_type": "text"
      },
      "source": [
        "### Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89R14WaJz0VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyNEe6RXYcU4",
        "colab_type": "text"
      },
      "source": [
        "### Adding the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngNWQHVl0Bc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4u2S1_Y4WG",
        "colab_type": "text"
      },
      "source": [
        "## Part 3 - Training the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GWlJChhY_ZI",
        "colab_type": "text"
      },
      "source": [
        "### Compiling the ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEzd8Zmz1NDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.compile(optimizer='adam', loss= 'binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QR_G5u7ZLSM",
        "colab_type": "text"
      },
      "source": [
        "### Training the ANN on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlCU20392clE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ed2931b-bffc-4f29-8e75-f55276ca6ba1"
      },
      "source": [
        "ann.fit(X, y, batch_size= 32, epochs= 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7767\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4819 - accuracy: 0.7991\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.8070\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4696 - accuracy: 0.8227\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.8193\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.8215\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8215\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.8204\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8227\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.8159\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.8193\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8215\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.8227\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8260\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.8238\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.8215\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.8249\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.8227\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8238\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.8215\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.8260\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8215\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.8238\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.8238\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.8272\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.8260\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.8204\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.8272\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.8260\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.8305\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8305\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.8305\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.8305\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8294\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.8294\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8316\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8260\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.8294\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4413 - accuracy: 0.8283\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8283\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8305\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8294\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8328\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8294\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8294\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.8272\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8305\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8294\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.8316\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8238\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8305\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8305\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8294\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8305\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8305\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8316\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8328\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8328\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8328\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.8350\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8316\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8283\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8328\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8316\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8316\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8339\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8316\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8339\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.8316\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8328\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8328\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8316\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8316\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.8294\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.8328\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4302 - accuracy: 0.8328\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.8328\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8361\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8339\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8339\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8328\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8339\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8328\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8328\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8350\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8316\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8339\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8316\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8350\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8328\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8328\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8328\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8350\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8361\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8339\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8373\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8361\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8316\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8339\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bcdfdb198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hCod89OQjpv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "6c8df035-a474-4ea5-a2b1-1b7a3d095d0f"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
              "       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
              "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJj5k2MxZga3",
        "colab_type": "text"
      },
      "source": [
        "## Part 4 - Making the predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7yx47jPZt11",
        "colab_type": "text"
      },
      "source": [
        "### Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJzoshovPr8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "189f9beb-6803-4f47-cf7b-fca008f20f68"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.73769513,  0.36944878, -0.4745452 , -0.47367361],\n",
              "       [-1.35557354,  1.33137817,  0.43279337, -0.47367361],\n",
              "       [ 0.73769513,  2.48569343, -0.4745452 , -0.47367361],\n",
              "       ...,\n",
              "       [ 0.73769513,  0.67726619, -0.4745452 , -0.47367361],\n",
              "       [ 0.73769513,  0.04413122, -0.4745452 , -0.47367361],\n",
              "       [ 0.73769513,  0.04413122,  0.43279337,  0.76762988]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj3ucISc7SpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}